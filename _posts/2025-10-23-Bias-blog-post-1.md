---
title: 'Bias and Fairness blog 1: Ai Regimes of Representation'
date: 2025-10-23
permalink: /posts/2025/10/Bias and Fairness-blog-post-1/
tags:
  - Bias
  - Fairness
  - AI
---
**Article:**  
[AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia](https://mit-serc.pubpub.org/pub/bfw5tscj/release/3?readingCollection=65a1a268)

Brief Summary

This case study presents a community-centered evaluation of South Asian cultural representations in text-to-image (T2I) models. Divng into inequality within Gen-AI models and ways that they can change.

What does cultural representation mean to you, and how might this definition and your experience with past representation impact how you would evaluate representation of your identity in generative AI models? What aspects of your identity do you think you would center when evaluating representations in AI model output?

To me, cultural representation means the ways that my culture is portayed in the news, movies, and social media. I beleive it reflects the who and why these stories are told and if they are accurate and respectful. Aspects of my identity that I think I would center when evaluating representations in AI model outputs include my race and gender. I am a black male in the United States which has systemically been seen as negative or less than. When evaluating generative AI models I would focus on how race and gender are depicted and how gen AI models priroitze those depictions or try to chanage the narrative. 

What do you think is the role of small-scale qualitative evaluations for building more ethical generative AI models? How do they compare to larger, quantitative, benchmark-style evaluations?

I think the role of small scale qualitative evaluations is to balance the unethical biases seen within larger models. Larger models overlook the cultutal, social, and economical apsects that these large models output. I beleive that small scale models almost expose the inequalites that large models are built around. I broke the words quantitative and qualitative down to quality and quantity which correlates to how the two models analyze frameworks. For example, in my own life, when I apply to jobs there is the demographic questions that are asked. For what reason are these questions asked? Does my race decide whter my application will be considered or is my name not one that is recognized by algorithms. 

Participants in this study shared “aspirations” for generative AI to be more inclusive, but also noted the tensions around making it more inclusive. Do you think AI can be made more globally inclusive?

I think that with how fast technlogy is evolving, AI can be made more inclusive if the right people are in charge of producing new LLMs. That is, people that are inclusive and open to all possibilites. Technology isn't cheap, people with money have the ability to invest in tech companies producing new LLMs which they then have a say in whether the models have biases or greater inclusion. However, I see where the participants are coming from and how “there’s no singular identity,” but rather “multiple languages, multiple cultures [...] and the complexities that come with that.” It is a large task at hand and different societies have different views on other societies so it could be a revolving circle of biases, just from different parts of the world, but I think it can be done.

What mitigations do you think developers could explore to respond to some of the concerns raised by participants in this study?

I think developers could allow users to know where data is coming from, as in who is programming the data and who is manipulating LLMs to output bias data so that they could be held reliable for lack of inclusive data, and users could also understand why certain large-scale models overlook certain demographics, genders, and people of certain classes. 

As mentioned in this case study, representation is not static; it changes over time, is culturally situated, and varies greatly from person to person. How can we “encode” this contextual and changing nature of representation into models, datasets, and algorithms? Is the idea of encoding at odds with the dynamism and fluidity of representation?

We can encode this change into models by moving to smaller scaled models. P22 mentions that “AI represents the majoritarian view, and if you’re someone who doesn’t fit in with that, then it’s particularly disturbing [for you].” Many individuals are categorized as somehting they are not which pushes a narrative onto the larger population  when they use LLMs about certain cultures and demographics. I think that encoding is at adds with the dynamism and fluidity of represenatation because models need structured inputs which moves away from the human aspect. However, an icrease in human input from a variety of demographics and models that are flexible can influence greater representation. 

How can we learn from the history of technology and media to build more responsible and representative AI models?

In the article it states "Insights about the politics of cultural technologies, particularly how they functioned in historically exclusionary ways to certain communities, shed light on how generative image technologies may have complicated relationships with communities historically marginalized from canonical, majoritarian representations." We can learn that historically generative image technologies lack inclusivity, so it is a mission to build more representative AI models that mitigate biases. In order to do this, there needs to be a change in leadership at certain companies, without someone in charge that is inclusive and open to new ideas, progress will be slow.

New Question: Task 3.
Do you think that finding local(counties, states, regions) stats and data then moving to a larger population rather than trying to incorporate multiple demographics at once will mitigate bias and lack of inclusion?

I chose this question because in the "tensions of inclusive generative AI" section of the article the participants mention how LLMs group individuals into one group and that programs opinions from outsiders. An example from P8 mentions how “Growing up, I was always categorized as ... [Indian]. I’m like no, I’m not Indian... No, I’m Bangladeshi. . . we have our own foods, we have our own holidays, we have our own historical events.” This shows that cultural identity can be lost when LLM's and outputs of T21 skew cultural representation.

Reflection:

I enjoyed reading about T21 models and the ways they impact cultural representation. I liked how the article incorporated the tensions of inclusive Gen Ai because it gave me a wider understanding on how solutions to underrepresentation can have its faults. Building from the ground up is emphasized so that individuals have the opportunity to contribute their values and culture into models which then could mitigate bias and strengthen inclusivity.
