---
title: 'Alg Blog 2: How GenAI Works'
date: 2025-09-30
permalink: /posts/2025/09/Alg-blog-post-2/
tags:
  - deep learning
  - GenAI
  - Ethics
---

How does Generative AI work and what causes failure? 

**News Article:**  
[How Generative AI Works and How It Fails](https://mit-serc.pubpub.org/pub/f3o5mpn6/release/1?readingCollection=3a6c54f1)

Summary: This case study goes in depth about the technical background of generative AI, while also covering failures and environmental risks of GenAI like deepfakes and unfair labor. 
1: Learning

I chose chat-gpt as my source for learning. I wanted to know about flying airplanes because that is something I plan on doing after college. I prompted the bot to tell me about the basics of flying and what someone would need to know before going into flight school. Chat broke down the different instruments that are used when flying and their purpose. It also talked about aircraft controls, basic principles, and the main maneuvers for basic flying. What worked well was chat's ability to give basic concepts of flying. I also noticed that this AI program offered follow-up questions each time I asked a question. I tested the chat-bot with some of the harder landing scenarios and it explained it well. It gave me techniques on how to land, what they are called, and what causes more difficult landing situations. Overall, I think this tool is helpful for basic understanding of concepts. A lot of the responses seem broad but you as the learner need to specify exactly what you want the chat-bot to output. I think Chat doesn't work well with complex math because I asked it to solve a navigation problem and it had some trouble so complex questions may not work the best. I think I would use chat-gpt for clarifying questions, I sometimes use chat-gpt for cooking instructions but I am sometimes hesitant, so building trust with AI is something that

2: Creative Work 

The article mentions how GenAI uses other artists, writers, photographers, and journalists to work without any credit, compensation, or consent. The ethics of this practice are wrong. Just because it's the GenAI taking this work from the internet and outputting it to the users, there is still responsibility on the backend. The developers and anyone involved in programming these models should go through the proper ethical practices when using other people's work. Those who want to change the system should do so by suing the companies that take their work. Encrypting data could also work. THere should be an agreement between these companies that allows these creators to be compensated without their work being stolen, which I think could be a step towards solving the bigger problem.

3: Next-word prediction

These large language models end up exhibiting a range of other capabilities because they're producing roughly a trillion arithmetic operations at a time when they're predicting next-words. The article states that this happens when a single "token" of a word is predicted. According to the citations mentioned in the article "fine tuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks." This relates to next-word prediction because with zero-shot performance no examples are needed in order perform a task, leading to more being said then the user expects. You see versions of AI that give you the option to recieve answers in depth with multiples examples and plenty of context or an option that is brief and straight-forward. 


4: Environmental impact

Connecting the reading to my own personal experience. When I worked at the Idaho National Laboratory they had a huge room of computers that were always running. The room was the size of the first floor of an office building and they mentioned that the amount of water they needed to keep the computers cool was more than we expect. The amount of water being used to keep these CPU's running could impact the environment in the U.S, taking away H2O from important resources. One of the articles mentions that in the united states, 34,000 wh are used per person while in the U.K. 12,000 wh are being used daily. Thus, plenty of energy is being used that could be generated to other sources. I branched out and read an article from MIT (https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117), that states multiple data centers are being built for big corporations and that would lead to more fossil fuels plants being built and more air and water being contaminated. We know that machine learning models must be trained, but at what cost? Another issue is the rapid fluctuations in energy use that occur during the training process. In order to keep these rapid fluctuations running power grids must use "diesal-based generators to keep the grids running, leading to environmental concerns. I think a question that arises for policy makers is how many data centers do we really need in order to keep AI running and updated. At some point these policymakers need to put the world's environment before AI but since AI is taking over our society there are hard conversations that need to be had.

My Question:

What are some ways that AI can still advance and not harm the environment? Following up, will there be a solution to protecting the environment while still building more data centers and fossil plants to allow these models to be trained? I chose this question because the environmental factors are the most important questions to ask when it comes to advancing AI.

Reflection

I think this assignment was informational. I think that the reading was a bit long but having the discussion questions at the end helped me navigate the reading and what I needed to look for. I learned about how AI fails and that helped me understand why sometimes there is AI fatigue when you prompt multiple questions. The code space process is easy to use now, as I mentioned in my last reflection. My assignment responses have been getting longer and more in depth, complimenting the density of the articles we read.

