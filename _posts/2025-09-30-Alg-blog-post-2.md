---
title: 'Alg Blog 2: How GenAI Works'
date: 2025-09-30
permalink: /posts/2025/09/Alg-blog-post-2/
tags:
  - deep learning
  - GenAI
  - Ethics
---

How does Generative AI work and what causes failure? 

**News Article:**  
[How Generative AI Works and How It Fails](https://mit-serc.pubpub.org/pub/f3o5mpn6/release/1?readingCollection=3a6c54f1)

1: Learning

I chose chat-gpt as my source for learning. I wanted to know about flying airplanes because that is something I plan on doing after college. I prompted the bot to tell me about the basics of flying and what someone would need to know before going into flight school. Chat broke down the different instruments that are used when flying and their purpose. It also talked about aircraft controls, basic principles, and the main maneuvers for basic flying. What worked well was chat's ability to give basic concepts of flying. I also noticed that this AI program offered follow-up questions each time I asked a question. I tested the chat-bot with some of the harder landing scenarios and it explained it well. It gave me techniques on how to land, what they are called, and what causes more difficult landing situations. Overall, I think this tool is helpful for basic understanding of concepts. A lot of the responses seem broad but you as the learner need to specify exactly what you want the chat-bot to output. I think Chat doesn't work well with complex math because I asked it to solve a navigation problem and it had some trouble so complex questions may not work the best. I think I would use chat-gpt for clarifying questions, I sometimes use chat-gpt for cooking instructions but I am sometimes hesitant, so building trust with AI is something that

2: Creative Work 

The article mentions how GenAI uses other artists, writers, photographers, and journalists to work without any credit, compensation, or consent. The ethics of this practice are wrong. Just because it's the GenAI taking this work from the internet and outputting it to the users, there is still responsibility on the backend. The developers and anyone involved in programming these models should go through the proper ethical practices when using other people's work. Those who want to change the system should do so by suing the companies that take their work. Encrypting data could also work. THere should be an agreement between these companies that allows these creators to be compensated without their work being stolen, which I think could be a step towards solving the bigger problem.

3: Next-word prediction

These large language models end up exhibiting a range of other capabilities because they're producing roughly a trillion arithmetic operations at a time when they're predicting next-words. The article states that this happens when a single "token" of a word is predicted. According to the citations mentioned in the article "fine tuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks." This relates to next-word prediction because with zero-shot performance no examples are needed in order perform a task, leading to more being said then the user expects. You see versions of AI that give you the option to recieve answers in depth with multiples examples and plenty of context or an option that is brief and straight-forward. 


4: Environmental impact


